{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Structuring your Tensorflow models\n",
    "\n",
    "Structure the linear model using class and lazy property decorator. Please refer to this [article](https://danijar.com/structuring-your-tensorflow-models/) by Danijar Hafner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "\n",
    "from mnist import MNIST\n",
    "data = MNIST(data_dir=\"data/MNIST/\")\n",
    "\n",
    "def lazy_property(function, name = None, *args, **kwargs):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    #name = scope or function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            #with tf.variable_scope(name, *args, **kwargs):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, input_size, output_size, image, target):\n",
    "        self.image = image\n",
    "        self.target = target\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = output_size\n",
    "        \n",
    "        self.logits\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.error\n",
    "        \n",
    "#   @lazy_property(initializer = tf.global_variable_initializer())\n",
    "    @lazy_property\n",
    "    def logits(self):\n",
    "        x = self.image\n",
    "        #weights = tf.Variable(tf.zeros([self.img_size_flat, self.num_classes]))\n",
    "        weights = tf.get_variable(name = 'weight', shape = [self.input_size, self.num_classes], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.Variable(tf.zeros([self.num_classes]))\n",
    "        self._logits = tf.matmul(x, weights) + biases\n",
    "        return self._logits\n",
    "        \n",
    "        \n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        ##### YOUR CODE START #####\n",
    "        self._prediction = tf.nn.softmax(self.logits)\n",
    "        return self._prediction\n",
    "        ##### YOUR CODE END #####\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        ##### YOUR CODE START #####\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.target)\n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        learning_rate = 0.1\n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        return self.optimizer\n",
    "        ##### YOUR CODE END #####\n",
    "    \n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        ##### YOUR CODE START #####\n",
    "        \"\"\" in previous code\n",
    "        correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "        \n",
    "        \"\"\"\n",
    "        y_pred_cls = tf.argmax(self.prediction, axis = 1)\n",
    "        y_true_cls = tf.argmax(self.target, axis = 1)\n",
    "        wrong_prediction = tf.not_equal(y_pred_cls, y_true_cls)\n",
    "        self._error = tf.reduce_mean(tf.cast(wrong_prediction, tf.float32))\n",
    "        return self._error\n",
    "        ##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate @ iter 0: [Train] 0.590000  [Test] 0.744200\n",
      "Error rate @ iter 100: [Train] 0.140000  [Test] 0.114400\n",
      "Error rate @ iter 200: [Train] 0.110000  [Test] 0.099900\n",
      "Error rate @ iter 300: [Train] 0.080000  [Test] 0.094800\n",
      "Error rate @ iter 400: [Train] 0.110000  [Test] 0.090100\n",
      "Error rate @ iter 500: [Train] 0.110000  [Test] 0.088900\n",
      "Error rate @ iter 600: [Train] 0.070000  [Test] 0.086700\n",
      "Error rate @ iter 700: [Train] 0.070000  [Test] 0.087200\n",
      "Error rate @ iter 800: [Train] 0.070000  [Test] 0.087100\n",
      "Error rate @ iter 900: [Train] 0.080000  [Test] 0.085300\n",
      "Error rate @ iter 1000: [Train] 0.080000  [Test] 0.083900\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_steps = 1000\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO : Model object construction\n",
    "img_size_flat = 28 * 28\n",
    "num_classes = 10\n",
    "image = tf.placeholder(tf.float32, [None, img_size_flat])\n",
    "target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "model = Model(img_size_flat, num_classes, image, target)\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_steps+1):\n",
    "        # TODO : Model Optimization\n",
    "        x_batch, y_batch, _ = data.random_batch(batch_size = batch_size)\n",
    "        feed_dict_train = {image : x_batch, target : y_batch}\n",
    "        feed_dict_test = {image : data.x_test, target : data.y_test}\n",
    "        session.run(model.optimize, feed_dict_train)\n",
    "        train_error = session.run(model.error, feed_dict_train)\n",
    "        test_error = session.run(model.error, feed_dict_test)\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print(\"Error rate @ iter %d: [Train] %f  [Test] %f\" % (step, train_error, test_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Trial (Optimizer : AdagradOptimizer / Learning rate : 0.1 / batch_size : 100\n",
    "\n",
    "Error rate @ iter 0: [Train] 0.750000  [Test] 0.841800 <br>\n",
    "Error rate @ iter 100: [Train] 0.110000  [Test] 0.110700 <br>\n",
    "Error rate @ iter 200: [Train] 0.110000  [Test] 0.099000 <br>\n",
    "Error rate @ iter 300: [Train] 0.120000  [Test] 0.096200 <br>\n",
    "Error rate @ iter 400: [Train] 0.080000  [Test] 0.089700 <br>\n",
    "Error rate @ iter 500: [Train] 0.100000  [Test] 0.086800 <br>\n",
    "Error rate @ iter 600: [Train] 0.060000  [Test] 0.087600 <br>\n",
    "Error rate @ iter 700: [Train] 0.050000  [Test] 0.085400 <br>\n",
    "Error rate @ iter 800: [Train] 0.140000  [Test] 0.085800 <br>\n",
    "Error rate @ iter 900: [Train] 0.100000  [Test] 0.084200 <br>\n",
    "Error rate @ iter 1000: [Train] 0.060000  [Test] 0.081000 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Sequential trial\n",
    "optimizer : AdagradOptimizer / batch_size : 1000 / loop : 100 / test_acc : 0.9236000180244446 <br>\n",
    "optimizer : AdagradOptimizer / batch_size : 1000 / loop : 100 / train_acc : 0.9253454804420471 / test_acc : 0.9246000051498413  <br>\n",
    "optimizer : GradientDescentOptimizer / batch_size : 1000 / loop : 100 / train_acc : 0.9179999828338623 / test_acc : 0.9194999933242798  <br>\n",
    "optimizer : GradientDescentOptimizer / batch_size : 1000 / loop : 100 / train_acc : 91.929% / test_acc : 92.010%  <br>\n",
    "optimizer : GradientDescentOptimizer / learning_rate : 0.5 / batch_size : 1000 / loop : 100 / train_acc : 90.645% / test_acc : 89.840%  <br>\n",
    "optimizer : GradientDescentOptimizer / learning_rate : 0.5 / batch_size : 1000 / loop : 100 / train_acc : 89.878% / test_acc : 89.130%  <br>\n",
    "optimizer : GradientDescentOptimizer / learning_rate : 0.1 / batch_size : 1000 / loop : 100 / train_acc : 92.411% / test_acc : 91.400%  <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials with weight initialization(Xavier Initialization)\n",
    "\n",
    "Error rate @ iter 0: [Train] 0.590000  [Test] 0.744200 <br>\n",
    "Error rate @ iter 100: [Train] 0.140000  [Test] 0.114400 <br>\n",
    "Error rate @ iter 200: [Train] 0.110000  [Test] 0.099900 <br>\n",
    "Error rate @ iter 300: [Train] 0.080000  [Test] 0.094800 <br>\n",
    "Error rate @ iter 400: [Train] 0.110000  [Test] 0.090100 <br>\n",
    "Error rate @ iter 500: [Train] 0.110000  [Test] 0.088900 <br>\n",
    "Error rate @ iter 600: [Train] 0.070000  [Test] 0.086700 <br>\n",
    "Error rate @ iter 700: [Train] 0.070000  [Test] 0.087200 <br>\n",
    "Error rate @ iter 800: [Train] 0.070000  [Test] 0.087100 <br>\n",
    "Error rate @ iter 900: [Train] 0.080000  [Test] 0.085300 <br>\n",
    "Error rate @ iter 1000: [Train] 0.080000  [Test] 0.083900 <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
